{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9215ba",
   "metadata": {},
   "source": [
    "# Train a XGBoost regression model on Amazon SageMaker, host inference on a Docker container running on Amazon ECS on AWS Fargate and optionally expose as an API with Amazon API Gateway\n",
    "\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed end-to-end Machine Learning (ML) service. With SageMaker, you have the option of using the built-in algorithms or you can bring your own algorithms and frameworks to train your models.  After training, you can deploy the models in [one of two ways](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html) for inference - persistent endpoint or batch transform.\n",
    "\n",
    "With a persistent inference endpoint, you get a fully-managed real-time HTTPS endpoint hosted on either CPU or GPU based EC2 instances.  It supports features like auto scaling, data capture, model monitoring and also provides cost-effective GPU support using [Amazon Elastic Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html).  It also supports hosting multiple models using multi-model endpoints that provide A/B testing capability.  You can monitor the endpoint using [Amazon CloudWatch](https://aws.amazon.com/cloudwatch/).  In addition to all these, you can use [Amazon SageMaker Pipelines](https://aws.amazon.com/sagemaker/pipelines/) which provides a purpose-built, easy-to-use Continuous Integration and Continuous Delivery (CI/CD) service for Machine Learning.\n",
    "\n",
    "There are use cases where you may want to host the ML model on a real-time inference endpoint that is cost-effective and do not require all the capabilities provided by the SageMaker persistent inference endpoint.  These may involve,\n",
    "* simple models\n",
    "* models whose sizes are lesser than 200 MB\n",
    "* models that are invoked sparsely and do not need inference instances running all the time\n",
    "* models that do not need to be re-trained and re-deployed frequently\n",
    "* models that do not need GPUs for inference\n",
    "\n",
    "In these cases, you can take the trained ML model and host it on a container on [Amazon ECS on AWS Fargate](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html) and optionally expose it as an API by front-ending it with a HTTP/REST API hosted on [Amazon API Gateway](https://aws.amazon.com/api-gateway/).  This will be cost-effective as compared to having real-time inference instances and still provide a fully-managed and scalable solution.\n",
    "\n",
    "[Amazon Elastic Container Service (Amazon ECS)](https://aws.amazon.com/ecs) is a fully managed container orchestration service.  It is a highly scalable, fast container management service that makes it easy to run, stop and manage containers on a cluster. Your containers are defined in a task definition that you use to run individual tasks or tasks within a service.  In this context, a service is a configuration that enables you to run and maintain a specified number of tasks simultaneously in a cluster. You can run your tasks and services on a serverless infrastructure that is managed by [AWS Fargate](https://aws.amazon.com/fargate). Alternatively, for more control over your infrastructure, you can run your tasks and services on a cluster of Amazon EC2 instances that you manage.\n",
    "\n",
    "This notebook demonstrates this solution by using SageMaker's [built-in XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) to train a regression model on the [California Housing dataset](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html).  It loads the trained model as a Python3 [pickle](https://docs.python.org/3/library/pickle.html) object in a Python3 [Flask](https://flask.palletsprojects.com/en/1.1.x/) app script in a container to be hosted on [Amazon ECS on AWS Fargate](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html).  Finally, it provides instructions for exposing it as an API by front-ending it with a HTTP/REST API hosted on [Amazon API Gateway](https://aws.amazon.com/api-gateway/).\n",
    "\n",
    "**Warning:** The Python3 [pickle](https://docs.python.org/3/library/pickle.html) module is not secure.  Only unpickle data you trust.  Keep this in mind if you decide to get the trained ML model file from somewhere instead of building your own model.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "* This notebook should only be run from within a SageMaker notebook instance as it references SageMaker native APIs.  The underlying OS of the notebook instance can either be Amazon Linux v1 or v2.\n",
    "* At the time of writing this notebook, the most relevant latest version of the Jupyter notebook kernel for this notebook was `conda_python3` and this came built-in with SageMaker notebooks.\n",
    "* This notebook uses CPU based instances for training.\n",
    "* If you already have a trained model that can be loaded as a Python3 [pickle](https://docs.python.org/3/library/pickle.html) object, then you can skip the training step in this notebook and directly upload the model file to S3 and update the code in this notebook's cells accordingly.\n",
    "* In this notebook, the ML model generated in the training step has not been tuned as that is not the intent of this demo.\n",
    "* In this notebook, we will create only one ECS Task.  In order to scale to more tasks, you have to create an [Amazon ECS Service](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html) made up of multiple tasks.  You can then setup [Load Balancing](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html) and [Auto Scaling](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html).\n",
    "* This notebook will create resources in the same AWS account and in the same region where this notebook is running.\n",
    "\n",
    "**Table of Contents:**\n",
    "\n",
    "1. [Complete prerequisites](#Complete%20prerequisites)\n",
    "\n",
    "    1. [Check and configure access to the Internet](#Check%20and%20configure%20access%20to%20the%20Internet)\n",
    "\n",
    "    2. [Check and upgrade required software versions](#Check%20and%20upgrade%20required%20software%20versions)\n",
    "    \n",
    "    3. [Check and configure security permissions](#Check%20and%20configure%20security%20permissions)\n",
    "\n",
    "    4. [Organize imports](#Organize%20imports)\n",
    "    \n",
    "    5. [Create common objects](#Create%20common%20objects)\n",
    "\n",
    "2. [Prepare the data](#Prepare%20the%20data)\n",
    "\n",
    "    1. [Create the local directories](#Create%20the%20local%20directories)\n",
    "    \n",
    "    2. [Load the dataset and view the details](#Load%20the%20dataset%20and%20view%20the%20details)\n",
    "    \n",
    "    3. [(Optional) Visualize the dataset](#(Optional)%20Visualize%20the%20dataset)\n",
    "    \n",
    "    4. [Split the dataset into train, validate and test sets](#Split%20the%20dataset%20into%20train,%20validate%20and%20test%20sets)\n",
    "    \n",
    "    5. [Standardize the datasets](#Standardize%20the%20datasets)\n",
    "    \n",
    "    6. [Save the prepared datasets locally](#Save%20the%20prepared%20datasets%20locally)\n",
    "    \n",
    "    7. [Upload the prepared datasets to S3](#Upload%20the%20prepared%20datasets%20to%20S3)\n",
    "\n",
    "3. [Perform training](#Perform%20training)\n",
    "\n",
    "    1. [Set the training parameters](#Set%20the%20training%20parameters)\n",
    "    \n",
    "    2. [(Optional) Delete previous checkpoints](#(Optional)%20Delete%20previous%20checkpoints)\n",
    "    \n",
    "    3. [Run the training job](#Run%20the%20training%20job)\n",
    "\n",
    "4. [Create and push the Docker container to an Amazon ECR repository](#Create%20and%20push%20the%20Docker%20container%20to%20an%20Amazon%20ECR%20repository)\n",
    "\n",
    "    1. [Retrieve the model pickle file](#Retrieve%20the%20model%20pickle%20file)\n",
    "    \n",
    "    2. [(Optional) Test the model pickle file](#(Optional)%20Test%20the%20model%20pickle%20file)\n",
    "    \n",
    "    3. [View the inference script](#View%20the%20inference%20script)\n",
    "    \n",
    "    4. [Create the Dockerfile](#Create%20the%20Dockerfile)\n",
    "    \n",
    "    5. [Create the container](#Create%20the%20container)\n",
    "    \n",
    "    6. [Create the private repository in ECR](#Create%20the%20private%20repository%20in%20ECR)\n",
    "    \n",
    "    7. [Push the container to ECR](#Push%20the%20container%20to%20ECR)\n",
    "\n",
    "5. [Deploy and test on Amazon ECS on AWS Fargate](#Deploy%20and%20test%20on%20Amazon%20ECS%20on%20AWS%20Fargate)\n",
    "    \n",
    "    1. [Create the ECS cluster](#Create%20the%20ECS%20cluster)\n",
    "    \n",
    "    2. [Create the ECS Task and deploy the container](#Create%20the%20ECS%20Task%20and%20deploy%20the%20container)\n",
    "    \n",
    "    3. [Prepare to test the ECS Task](#Prepare%20to%20test%20the%20ECS%20Task)\n",
    "    \n",
    "    4. [Test the ECS Task](#Test%20the%20ECS%20Task)\n",
    "    \n",
    "6. [(Optional) Front-end the container with Amazon API Gateway](#(Optional)%20Front-end%20the%20container%20with%20Amazon%20API%20Gateway)\n",
    "\n",
    "7. [Cleanup](#Cleanup)\n",
    "\n",
    "    1. [Cleanup ECS resources](#Cleanup%20ECS%20resources)\n",
    "    \n",
    "    2. [Cleanup ECR repository](#Cleanup%20ECR%20repository)\n",
    "    \n",
    "    3. [Cleanup S3 objects](#Cleanup%20S3%20objects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c52881",
   "metadata": {},
   "source": [
    "##  1. Complete prerequisites <a id='Complete%20prerequisites'></a>\n",
    "\n",
    "Check and complete the prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8497a",
   "metadata": {},
   "source": [
    "###  A. Check and configure access to the Internet <a id='Check%20and%20configure%20access%20to%20the%20Internet'></a>\n",
    "\n",
    "This notebook requires outbound access to the Internet to download the required software updates and to make calls to the container hosted as an Amazon ECS Task.  You can either provide direct Internet access (default) or provide Internet access through a VPC.  For more information on this, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/appendix-notebook-and-internet-access.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac025687",
   "metadata": {},
   "source": [
    "### B. Check and upgrade required software versions  <a id='Check%20and%20upgrade%20required%20software%20versions'></a>\n",
    "\n",
    "This notebook requires:\n",
    "* [SageMaker Python SDK version 2.x](https://sagemaker.readthedocs.io/en/stable/v2.html)\n",
    "* [Python 3.6.x](https://www.python.org/downloads/release/python-360/)\n",
    "* [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
    "* [AWS Command Line Interface](https://aws.amazon.com/cli/)\n",
    "* [Docker](https://www.docker.com/)\n",
    "* [XGBoost Python module](https://xgboost.readthedocs.io/en/latest/python/python_intro.html)\n",
    "* [cURL](https://curl.se/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043df7a2",
   "metadata": {},
   "source": [
    "Capture the version of the OS on which this notebook is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from subprocess import Popen\n",
    "\n",
    "p = Popen(['cat','/etc/system-release'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)\n",
    "os_cmd_output, os_cmd_error = p.communicate()\n",
    "if len(os_cmd_error) > 0:\n",
    "    print('Notebook OS command returned error :: {}'.format(os_cmd_error))\n",
    "    os_version = ''\n",
    "else:\n",
    "    if os_cmd_output.find('Amazon Linux release 2') >= 0:\n",
    "        os_version = 'ALv2'\n",
    "    elif os_cmd_output.find('Amazon Linux AMI release 2018.03') >= 0:\n",
    "        os_version = 'ALv1'\n",
    "    else:\n",
    "        os_version = ''\n",
    "print('Notebook OS version : {}'.format(os_version))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad43109",
   "metadata": {},
   "source": [
    "**Note:** When running the following cell, if you get 'module not found' errors, then uncomment the appropriate installation commands and install the modules.  Also, uncomment and run the kernel shutdown command.  When the kernel comes back, comment out the installation and kernel shutdown commands and run the following cell.  Now, you should not see any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d74b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Last tested versions:\n",
    "\n",
    "\n",
    "On Amazon Linux v1 (ALv1) notebook:\n",
    "---------------------------\n",
    "SageMaker Python SDK version : 2.54.0\n",
    "Python version : 3.6.13 | packaged by conda-forge | (default, Feb 19 2021, 05:36:01) \n",
    "[GCC 9.3.0]\n",
    "Boto3 version : 1.18.27\n",
    "XGBoost Python module version : 1.4.2\n",
    "AWS CLI version : aws-cli/1.20.21 Python/3.6.13 Linux/4.14.238-125.422.amzn1.x86_64 botocore/1.21.27\n",
    "Docker version : 19.03.13-ce, build 4484c46\n",
    "\n",
    "\n",
    "On Amazon Linux v2 (ALv2) notebook:\n",
    "---------------------------\n",
    "SageMaker Python SDK version : 2.54.0\n",
    "Python version : 3.6.13 | packaged by conda-forge | (default, Feb 19 2021, 05:36:01) \n",
    "[GCC 9.3.0]\n",
    "Boto3 version : 1.18.27\n",
    "XGBoost Python module version : 1.4.2\n",
    "AWS CLI version : aws-cli/1.20.21 Python/3.6.13 Linux/4.14.238-125.422.amzn1.x86_64 botocore/1.21.27\n",
    "Docker version : 20.10.4, build d3cb89e\n",
    "Amazon ECR Docker Credential Helper : 0.6.3\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import IPython\n",
    "import os\n",
    "import sagemaker\n",
    "import sys\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ModuleNotFoundError:\n",
    "    # Install XGBoost and restart kernel\n",
    "    print('Installing XGBoost module...')\n",
    "    !{sys.executable} -m pip install -U xgboost\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "# Install/upgrade the Sagemaker SDK, Boto3 and XGBoost and restart kernel\n",
    "#!{sys.executable} -m pip install -U sagemaker boto3 xgboost\n",
    "#IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "# Get the current installed version of Sagemaker SDK, Python, Boto3 and XGBoost\n",
    "print('SageMaker Python SDK version : {}'.format(sagemaker.__version__))\n",
    "print('Python version : {}'.format(sys.version))\n",
    "print('Boto3 version : {}'.format(boto3.__version__))\n",
    "print('XGBoost Python module version : {}'.format(xgb.__version__))\n",
    "\n",
    "# Get the AWS CLI version\n",
    "print('AWS CLI version : ')\n",
    "!aws --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979990b8",
   "metadata": {},
   "source": [
    "**Docker:**\n",
    "\n",
    "Docker should be pre-installed in the SageMaker notebook instance.  Verify it by running the `docker --version` command.  If Docker is not installed, you can install it by uncommenting the install command in the following cell.  You will require `sudo` rights to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b7b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if docker is installed\n",
    "!docker --version\n",
    "\n",
    "# Install docker\n",
    "#!sudo yum install docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edff95a4",
   "metadata": {},
   "source": [
    "**cURL:**\n",
    "\n",
    "cURL should be pre-installed in the SageMaker notebook instance.  Verify it by running the `curl --version` command.  If cURL is not installed, you can install it by uncommenting the install command in the following cell.  You will require `sudo` rights to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Last tested version:\n",
    "curl 7.71.1 (x86_64-conda-linux-gnu) libcurl/7.71.1 OpenSSL/1.1.1j zlib/1.2.11 libssh2/1.9.0 nghttp2/1.43.0\n",
    "Release-Date: 2020-07-01\n",
    "Protocols: dict file ftp ftps gopher http https imap imaps pop3 pop3s rtsp scp sftp smb smbs smtp smtps telnet tftp \n",
    "Features: AsynchDNS GSS-API HTTP2 HTTPS-proxy IPv6 Kerberos Largefile libz NTLM NTLM_WB SPNEGO SSL TLS-SRP UnixSockets\n",
    "\"\"\"\n",
    "\n",
    "# Verify if curl is installed\n",
    "!curl --version\n",
    "\n",
    "# Install curl\n",
    "#!sudo yum install curl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea03a55e",
   "metadata": {},
   "source": [
    "**Additional prerequisite (when notebook is running on Amazon Linux v2):**\n",
    "\n",
    "Install and configure the [Amazon ECR credential helper](https://github.com/awslabs/amazon-ecr-credential-helper).  This makes it easier to store and use Docker credentials for use with Amazon ECR private registries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253df7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os_version == 'ALv2':\n",
    "    # Install\n",
    "    !sudo yum --assumeyes install amazon-ecr-credential-helper\n",
    "    # Verify installation\n",
    "    print('Amazon ECR Docker Credential Helper version : ')\n",
    "    !docker-credential-ecr-login version\n",
    "    # Configure\n",
    "    !printf \"{\\\\n\\\\t\\\"credsStore\\\": \\\"ecr-login\\\"\\\\n}\" > ~/.docker/config.json\n",
    "    # Verify configuration\n",
    "    !cat ~/.docker/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781f493",
   "metadata": {},
   "source": [
    "###  C. Check and configure security permissions <a id='Check%20and%20configure%20security%20permissions'></a>\n",
    "\n",
    "This notebook uses the IAM role attached to the underlying notebook instance.  This role should have the following permissions,\n",
    "\n",
    "1. Full access to the S3 bucket that will be used to store training and output data.\n",
    "2. Full access to launch training instances.\n",
    "3. Access to create CloudWatch Log Groups.\n",
    "4. Access to write to CloudWatch Logs and CloudWatch Metrics.\n",
    "5. Access to create, delete and write to Amazon ECR private registries.\n",
    "6. Access to create and delete Amazon ECS clusters and a task definitions.\n",
    "7. Access to run ECS tasks.\n",
    "\n",
    "To view the name of this role, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3423f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sagemaker.get_execution_role())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8434e2fa",
   "metadata": {},
   "source": [
    "This notebook creates a [task](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html) on [Amazon ECS on AWS Fargate](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html).  This task requires an IAM role named 'Task Execution IAM role' that it assumes when it is invoked.  For more information on this, refer [here](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_execution_IAM_role.html).\n",
    "\n",
    "For the task created in this notebook, at a minimum, this role should provide access to the following,\n",
    "\n",
    "* Access to create CloudWatch Log Groups.\n",
    "* Access to write to CloudWatch Logs and CloudWatch Metrics.\n",
    "* Read access to Amazon ECR.\n",
    "\n",
    "For information on the various IAM roles required for Amazon ECS on AWS Fargate refer [here](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d360c",
   "metadata": {},
   "source": [
    "###  D. Organize imports <a id='Organize%20imports'></a>\n",
    "\n",
    "Organize all the library and module imports for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import seaborn as sns\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tarfile\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20486e11",
   "metadata": {},
   "source": [
    "###  E. Create common objects <a id='Create%20common%20objects'></a>\n",
    "\n",
    "Create common objects to be used in future steps in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c3386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the S3 bucket name\n",
    "s3_bucket = '<Specify the S3 bucket name>'\n",
    "\n",
    "# Create the S3 Boto3 resource\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_bucket_resource = s3_resource.Bucket(s3_bucket)\n",
    "\n",
    "# Create the SageMaker Boto3 client\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "# Create the ECR client\n",
    "ecr_client = boto3.client('ecr')\n",
    "\n",
    "# Create the Amazon ECS client\n",
    "ecs_client = boto3.client('ecs')\n",
    "\n",
    "# Create the Amazon EC2 client\n",
    "ec2_client = boto3.client('ec2')\n",
    "\n",
    "# Get the AWS region name\n",
    "region_name = sagemaker.Session().boto_region_name\n",
    "\n",
    "# Base name to be used to create resources\n",
    "nb_name = 'sm-xgboost-ca-housing-ecs-container-model-hosting'\n",
    "\n",
    "# Names of various resources\n",
    "train_job_name = 'train-{}'.format(nb_name)\n",
    "\n",
    "# Names of local sub-directories in the notebook file system\n",
    "data_dir = os.path.join(os.getcwd(), 'data/{}'.format(nb_name))\n",
    "train_dir = os.path.join(os.getcwd(), 'data/{}/train'.format(nb_name))\n",
    "val_dir = os.path.join(os.getcwd(), 'data/{}/validate'.format(nb_name))\n",
    "test_dir = os.path.join(os.getcwd(), 'data/{}/test'.format(nb_name))\n",
    "\n",
    "# Location of the datasets file in the notebook file system\n",
    "dataset_csv_file = os.path.join(os.getcwd(), 'datasets/california_housing.csv')\n",
    "\n",
    "# Container artifacts directory in the notebook file system\n",
    "container_artifacts_dir = os.path.join(os.getcwd(), 'container-artifacts/{}'.format(nb_name))\n",
    "\n",
    "# Location of the Python3 Flask script (containing the inference code) and it's corresponding\n",
    "# requirements.txt in the notebook file system\n",
    "container_script_file_name = 'container_sm_xgboost_ca_housing_inference.py'\n",
    "container_script_req_file_name = 'container_sm_xgboost_ca_housing_inference_requirements.txt'\n",
    "container_script_file = os.path.join(os.getcwd(), 'scripts/{}'.format(container_script_file_name))\n",
    "container_script_req_file = os.path.join(os.getcwd(), 'scripts/{}'.format(container_script_req_file_name))\n",
    "\n",
    "# Sub-folder names in S3\n",
    "train_dir_s3_prefix = '{}/data/train'.format(nb_name)\n",
    "val_dir_s3_prefix = '{}/data/validate'.format(nb_name)\n",
    "test_dir_s3_prefix = '{}/data/test'.format(nb_name)\n",
    "\n",
    "# Location in S3 where the model checkpoint will be stored\n",
    "model_checkpoint_s3_path = 's3://{}/{}/checkpoint/'.format(s3_bucket, nb_name)\n",
    "\n",
    "# Location in S3 where the trained model will be stored\n",
    "model_output_s3_path = 's3://{}/{}/output/'.format(s3_bucket, nb_name)\n",
    "\n",
    "# Names of the model tar file and extracted file - these are dependent on the\n",
    "# framework and algorithm you used to train the model.  This notebook uses\n",
    "# SageMaker's built-in XGBoost algorithm and that will have the names as follows:\n",
    "model_tar_file_name = 'model.tar.gz'\n",
    "extracted_model_file_name = 'xgboost-model'\n",
    "\n",
    "# Container details\n",
    "container_image_name = nb_name\n",
    "container_registry_url_prefix = '<Specify the ECR URL prefix in this format {aws_account_id}.dkr.ecr.{region}.amazonaws.com>'\n",
    "\n",
    "# ECS cluster details\n",
    "ecs_cluster_name = 'cluster-{}'.format(nb_name)\n",
    "\n",
    "# ECS Task details\n",
    "ecs_fargate_task_name = 'fargate-task-{}'.format(nb_name)\n",
    "ecs_fargate_task_role = '<Specify the ARN for the ECS Task IAM role>'\n",
    "ecs_fargate_task_execution_role = '<Specify the ARN for the ECS Task execution IAM role>'\n",
    "ecs_fargate_task_cpu = '0.25 vCPU'\n",
    "ecs_fargate_task_memory = '0.5 GB'\n",
    "ecs_fargate_task_count = 1\n",
    "\n",
    "# ECS Task networking details\n",
    "ecs_fargate_task_subnet_list = ['<Specify the ID of a public subnet in your preferred VPC>']\n",
    "ecs_fargate_task_security_group_list = ['<Specify the ID of your Security Group in your preferred VPC>']\n",
    "\n",
    "# ECS Task container details\n",
    "ecs_container_name = 'container-{}'.format(nb_name)\n",
    "ecs_container_port = 80\n",
    "ecs_container_host_port = 80\n",
    "ecs_container_healthcheck_command_list = [\"CMD-SHELL\", \"curl -f http://localhost:80/healthcheck || exit 1\"]\n",
    "ecs_container_healthcheck_interval_in_seconds = 30\n",
    "ecs_container_healthcheck_timeout_in_seconds = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62163697",
   "metadata": {},
   "source": [
    "## 2. Prepare the data <a id='Prepare%20the%20data'></a>\n",
    "\n",
    "The [California Housing dataset](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html) consists of 20,640 observations on housing prices with 9 economic covariates.  These covariates are,\n",
    "\n",
    "* MedianHouseValue\n",
    "* MedianIncome\n",
    "* HousingMedianAge\n",
    "* TotalRooms\n",
    "* TotalBedrooms\n",
    "* Population\n",
    "* Households\n",
    "* Latitude\n",
    "* Longitude\n",
    "\n",
    "This dataset has been downloaded to the local `datasets` directory and modified as a CSV file with the feature names in the first row.  This will be used in this notebook.\n",
    "\n",
    "The following steps will help with preparing the datasets for training, validation and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8f2e2",
   "metadata": {},
   "source": [
    "### A) Create the local directories <a id='Create%20the%20local%20directories'></a>\n",
    "\n",
    "Create the directories in the local system where the dataset will be copied to and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the local directories if they don't exist\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f8e37",
   "metadata": {},
   "source": [
    "### B) Load the dataset and view the details <a id='Load%20the%20dataset%20and%20view%20the%20details'></a>\n",
    "\n",
    "Check if the CSV file exists in the `datasets` directory and load it into a Pandas DataFrame.  Finally, print the details of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ae143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset file exists and proceed\n",
    "if os.path.exists(dataset_csv_file):\n",
    "    print('Dataset CSV file \\'{}\\' exists.'.format(dataset_csv_file))\n",
    "    # Load the data into a Pandas DataFrame\n",
    "    pd_data_frame = pd.read_csv(dataset_csv_file)\n",
    "    # Print the first 5 records\n",
    "    #print(pd_data_frame.head(5))\n",
    "    # Describe the dataset\n",
    "    print(pd_data_frame.describe())\n",
    "else:\n",
    "    print('Dataset CSV file \\'{}\\' does not exist.'.format(dataset_csv_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f89eac",
   "metadata": {},
   "source": [
    "### C) (Optional) Visualize the dataset <a id='(Optional)%20Visualize%20the%20dataset'></a>\n",
    "\n",
    "Display the distributions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the correlation matrix\n",
    "plt.figure(figsize=(11, 7))\n",
    "sns.heatmap(cbar=False, annot=True, data=(pd_data_frame.corr() * 100), cmap='coolwarm')\n",
    "plt.title('% Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30499748",
   "metadata": {},
   "source": [
    "### D) Split the dataset into train, validate and test sets <a id='Split%20the%20dataset%20into%20train,%20validate%20and%20test%20sets'></a>\n",
    "\n",
    "Split the dataset into train, validate and test sets after shuffling.  Split further into x and y sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63783a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test datasets after shuffling\n",
    "train, test = sklearn.model_selection.train_test_split(pd_data_frame, test_size=0.2,\n",
    "                                                       random_state=35, shuffle=True)\n",
    "# Split the train dataset further into train and validation datasets after shuffling\n",
    "train, val = sklearn.model_selection.train_test_split(train, test_size=0.1,\n",
    "                                                      random_state=25, shuffle=True)\n",
    "\n",
    "# Define functions to get x and y columns\n",
    "def get_x(df):\n",
    "    return df[['median_income','housing_median_age','total_rooms','total_bedrooms',\n",
    "                 'population','households','latitude','longitude']]\n",
    "def get_y(df):\n",
    "    return df[['median_house_value']]\n",
    "\n",
    "# Load the x and y columns for train, validation and test datasets\n",
    "x_train = get_x(train)\n",
    "y_train = get_y(train)\n",
    "x_val = get_x(val)\n",
    "y_val = get_y(val)\n",
    "x_test = get_x(test)\n",
    "y_test = get_y(test)\n",
    "\n",
    "# Summarize the datasets\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b202f6",
   "metadata": {},
   "source": [
    "### E) Standardize the datasets <a id='Standardize%20the%20datasets'></a>\n",
    "\n",
    "* Standardize the x columns of the train dataset using the `fit_transform()` function of `StandardScaler`.\n",
    "* Standardize the x columns of the validate and test datasets using the `transform()` function of `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a68390",
   "metadata": {},
   "source": [
    "### F) Save the prepared datasets locally <a id='Save%20the%20prepared%20datasets%20locally'></a>\n",
    "\n",
    "Save the prepared train, validate and test datasets to local directories.  Prior to saving, concatenate x and y columns as needed.  Create the directories if they don't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the prepared dataset (in numpy format) to the local directories as csv files\n",
    "\n",
    "np.savetxt(os.path.join(train_dir, 'train.csv'),\n",
    "           np.concatenate((y_train.to_numpy(), x_train), axis=1), delimiter=',')\n",
    "np.savetxt(os.path.join(train_dir, 'train_x.csv'), x_train)\n",
    "np.savetxt(os.path.join(train_dir, 'train_y.csv'), y_train.to_numpy())\n",
    "\n",
    "np.savetxt(os.path.join(val_dir, 'validate.csv'),\n",
    "           np.concatenate((y_val.to_numpy(), x_val), axis=1), delimiter=',')\n",
    "np.savetxt(os.path.join(val_dir, 'validate_x.csv'), x_val)\n",
    "np.savetxt(os.path.join(val_dir, 'validate_y.csv'), y_val.to_numpy())\n",
    "\n",
    "np.savetxt(os.path.join(test_dir, 'test.csv'),\n",
    "           np.concatenate((y_test.to_numpy(), x_test), axis=1), delimiter=',')\n",
    "np.savetxt(os.path.join(test_dir, 'test_x.csv'), x_test)\n",
    "np.savetxt(os.path.join(test_dir, 'test_y.csv'), y_test.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15152e",
   "metadata": {},
   "source": [
    "### G) Upload the prepared datasets to S3 <a id='Upload%20the%20prepared%20datasets%20to%20S3'></a>\n",
    "\n",
    "Upload the datasets from the local directories to appropriate sub-directories in the specified S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4274f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the data to S3\n",
    "train_dir_s3_path = sagemaker.Session().upload_data(path='./data/{}/train/'.format(nb_name),\n",
    "                                                          bucket=s3_bucket,\n",
    "                                                          key_prefix=train_dir_s3_prefix)\n",
    "val_dir_s3_path = sagemaker.Session().upload_data(path='./data/{}/validate/'.format(nb_name),\n",
    "                                                        bucket=s3_bucket,\n",
    "                                                        key_prefix=val_dir_s3_prefix)\n",
    "test_dir_s3_path = sagemaker.Session().upload_data(path='./data/{}/test/'.format(nb_name),\n",
    "                                                         bucket=s3_bucket,\n",
    "                                                         key_prefix=test_dir_s3_prefix)\n",
    "\n",
    "# Capture the S3 locations of the uploaded datasets\n",
    "train_s3_path = '{}/train.csv'.format(train_dir_s3_path)\n",
    "train_x_s3_path = '{}/train_x.csv'.format(train_dir_s3_path)\n",
    "train_y_s3_path = '{}/train_y.csv'.format(train_dir_s3_path)\n",
    "val_s3_path = '{}/validate.csv'.format(val_dir_s3_path)\n",
    "val_x_s3_path = '{}/validate_x.csv'.format(val_dir_s3_path)\n",
    "val_y_s3_path = '{}/validate_y.csv'.format(val_dir_s3_path)\n",
    "test_s3_path = '{}/test.csv'.format(test_dir_s3_path)\n",
    "test_x_s3_path = '{}/test_x.csv'.format(test_dir_s3_path)\n",
    "test_y_s3_path = '{}/test_y.csv'.format(test_dir_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88898d5",
   "metadata": {},
   "source": [
    "##  3. Perform training <a id='Perform%20training'></a>\n",
    "\n",
    "In this step, SageMaker's [built-in XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) is used to train a regression model on the [California Housing dataset](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html).\n",
    "\n",
    "Note: This model has not been tuned as that is not the intent of this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fff7a7",
   "metadata": {},
   "source": [
    "### A) Set the training parameters <a id='Set%20the%20training%20parameters'></a>\n",
    "\n",
    "1. Inputs - S3 location of the training and validation data.\n",
    "2. Hyperparameters.\n",
    "3. Training instance details:\n",
    "\n",
    "    1. Instance count\n",
    "    \n",
    "    2. Instance type\n",
    "    \n",
    "    3. The max run time of the training job\n",
    "    \n",
    "    4. (Optional) Use Spot instances.  For more info, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html).\n",
    "    \n",
    "    5. (Optional) The max wait for Spot instances, if using Spot.  This should be larger than the max run time.\n",
    "    \n",
    "4. Base job name\n",
    "5. Appropriate local and S3 directories that will be used by the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286dfd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input data input along with their content types\n",
    "train_input = TrainingInput(train_s3_path, content_type='text/csv')\n",
    "val_input = TrainingInput(val_s3_path, content_type='text/csv')\n",
    "inputs = {'train':train_input, 'validation':val_input}\n",
    "\n",
    "# Set the hyperparameters\n",
    "hyperparameters = {\n",
    "        'objective':'reg:squarederror',\n",
    "        'max_depth':'6',\n",
    "        'eta':'0.3',\n",
    "        'alpha':'3',\n",
    "        'colsample_bytree':'0.7',\n",
    "        'num_round':'100'}\n",
    "\n",
    "# Set the instance count, instance type, volume size, options to use Spot instances and other parameters\n",
    "train_instance_count = 1\n",
    "train_instance_type = 'ml.m5.xlarge'\n",
    "train_instance_volume_size_in_gb = 5\n",
    "#use_spot_instances = True\n",
    "#spot_max_wait_time_in_seconds = 5400\n",
    "use_spot_instances = False\n",
    "spot_max_wait_time_in_seconds = None\n",
    "max_run_time_in_seconds = 3600\n",
    "algorithm_name = 'xgboost'\n",
    "algorithm_version = '1.2-1'\n",
    "py_version = 'py37'\n",
    "# Get the container image URI for the specified parameters\n",
    "container_image_uri = sagemaker.image_uris.retrieve(framework=algorithm_name,\n",
    "                                                    region=region_name,\n",
    "                                                    version=algorithm_version,\n",
    "                                                    py_version=py_version,\n",
    "                                                    instance_type=train_instance_type,\n",
    "                                                    image_scope='training')\n",
    "\n",
    "# Set the training container related parameters\n",
    "container_log_level = logging.INFO\n",
    "\n",
    "# Location where the model checkpoints will be stored locally in the container before being uploaded to S3\n",
    "model_checkpoint_local_dir = '/opt/ml/checkpoints/'\n",
    "\n",
    "# Location where the trained model will be stored locally in the container before being uploaded to S3\n",
    "model_local_dir = '/opt/ml/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af5467",
   "metadata": {},
   "source": [
    "### B) (Optional) Delete previous checkpoints <a id='(Optional)%20Delete%20previous%20checkpoints'></a>\n",
    "\n",
    "If model checkpoints from previous trainings are found in the S3 checkpoint location specified in the previous step, then training will resume from those checkpoints.  In order to start a fresh training, run the following code cell to delete all checkpoint objects from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e3e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the checkpoints if you want to train from the beginning; else ignore this code cell\n",
    "for checkpoint_file in s3_bucket_resource.objects.filter(Prefix='{}/checkpoint/'.format(nb_name)):\n",
    "    checkpoint_file_key = checkpoint_file.key\n",
    "    print('Deleting {} ...'.format(checkpoint_file_key))\n",
    "    s3_resource.Object(s3_bucket_resource.name, checkpoint_file_key).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd0522",
   "metadata": {},
   "source": [
    "### C) Run the training job <a id='Run%20the%20training%20job'></a>\n",
    "\n",
    "Prepare the `estimator` and call the `fit()` method.  This will pull the container containing the specified version of the algorithm in the AWS region and run the training job in the specified type of EC2 instance(s).  The training data will be pulled from the specified location in S3 and training results and checkpoints will be written to the specified locations in S3.\n",
    "\n",
    "Note: SageMaker Debugger is disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa94aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the estimator\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=container_image_uri,\n",
    "    checkpoint_local_path=model_checkpoint_local_dir,\n",
    "    checkpoint_s3_uri=model_checkpoint_s3_path,\n",
    "    model_dir=model_local_dir,\n",
    "    output_path=model_output_s3_path,\n",
    "    instance_type=train_instance_type,\n",
    "    instance_count=train_instance_count,\n",
    "    use_spot_instances=use_spot_instances,\n",
    "    max_wait=spot_max_wait_time_in_seconds,\n",
    "    max_run=max_run_time_in_seconds,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    base_job_name=train_job_name,\n",
    "    framework_version=algorithm_version,\n",
    "    py_version=py_version,\n",
    "    container_log_level=container_log_level,\n",
    "    script_mode=False,\n",
    "    debugger_hook_config=False,\n",
    "    disable_profiler=True)\n",
    "\n",
    "# Perform the training\n",
    "estimator.fit(inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d8b7e",
   "metadata": {},
   "source": [
    "##  4. Create and push the Docker container to an Amazon ECR repository <a id='Create%20and%20push%20the%20Docker%20container%20to%20an%20Amazon%20ECR%20repository'></a>\n",
    "\n",
    "In this step, we will create a Docker container containing the generated model along with its dependencies.  If you bring a pre-trained model, you can upload it to S3 and use it to build the container.  The following steps contains instructions for doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48d741",
   "metadata": {},
   "source": [
    "### A) Retrieve the model pickle file <a id='Retrieve%20the%20model%20pickle%20file'></a>\n",
    "\n",
    "* The model file generated using SageMaker's [built-in XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) will be a Python pickle file zipped up in a tar file named `model.tar.gz`.  The S3 URI for this file will be available in the `model_data` attribute of the `estimator` object created in the training step.\n",
    "\n",
    "* If you bring your pre-trained model, you have to specify the S3 URI appropriately in the following cell.\n",
    "\n",
    "* The zip file needs to be downloaded from S3 and extracted.\n",
    "\n",
    "* The name of the extracted pickle file will depend on the framework and algorithm that was used to train the model.  In this notebook example, we have used SageMaker's [built-in XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) and so the pickle file will be named `xgboost-model`.  You will see this when the model tar file is extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the container artifacts directory if it doesn't exist\n",
    "os.makedirs(container_artifacts_dir, exist_ok=True)\n",
    "\n",
    "# Set the file paths\n",
    "model_tar_file_s3_path_suffix = '{}/output/{}/output/{}'.format(nb_name,\n",
    "                                                                estimator.latest_training_job.name,\n",
    "                                                                model_tar_file_name)\n",
    "model_tar_file_local_path = '{}/{}'.format(container_artifacts_dir, model_tar_file_name)\n",
    "extracted_model_file_local_path = '{}/{}'.format(container_artifacts_dir, extracted_model_file_name)\n",
    "\n",
    "# Delete old model files if they exist\n",
    "if os.path.exists(model_tar_file_local_path):\n",
    "    os.remove(model_tar_file_local_path)\n",
    "if os.path.exists(extracted_model_file_local_path):\n",
    "    os.remove(extracted_model_file_local_path)\n",
    "\n",
    "# Download the model tar file from S3\n",
    "s3_bucket_resource.download_file(model_tar_file_s3_path_suffix, model_tar_file_local_path)\n",
    "\n",
    "# Extract the model tar file and retrieve the model pickle file\n",
    "with tarfile.open(model_tar_file_local_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=container_artifacts_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd413023",
   "metadata": {},
   "source": [
    "### B) (Optional) Test the model pickle file <a id='(Optional)%20Test%20the%20model%20pickle%20file'></a>\n",
    "\n",
    "The code in the following cell entirely depends on the framework and algorithm that was used to train the model.  The extracted Python3 pickle file will contain the appropriate object name.  If you are bringing your own model file, you have to change this cell appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e7851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model pickle file as a pickle object\n",
    "pickle_file_path = extracted_model_file_local_path\n",
    "with open(pickle_file_path, 'rb') as pkl_file:\n",
    "    model = pickle.load(pkl_file)\n",
    "\n",
    "# Run a prediction against the model loaded as a pickle object\n",
    "# by sending the first record of the test dataset\n",
    "test_pred_x_df = pd.read_csv(StringIO(','.join(map(str, x_test[0]))), sep=',', header=None)\n",
    "test_pred_x = xgb.DMatrix(test_pred_x_df.values)\n",
    "print('Input for prediction = {}'.format(test_pred_x_df.values))\n",
    "print('Predicted value = {}'.format(model.predict(test_pred_x)[0]))\n",
    "print('Actual value = {}'.format(y_test.values[0][0]))\n",
    "print('Note: There may be a huge difference between the actual and predicted values as the model has not been tuned in the training step.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1597cb",
   "metadata": {},
   "source": [
    "### C) View the inference script <a id='View%20the%20inference%20script'></a>\n",
    "\n",
    "The inference script is a Python3 [Flask](https://flask.palletsprojects.com/en/1.1.x/) app script that contains the following logic:\n",
    "* Initialize the Flask web app server.\n",
    "* Load the ML model pickle object into memory.\n",
    "* Run the Flask web app server.\n",
    "* Parse the request sent to the web app server either from direct invocation or from a REST/HTTP API in Amazon API Gateway.\n",
    "* Run the prediction.\n",
    "* Format the response to match with the parameter specified in the request.\n",
    "* Return the response.\n",
    "* Implement the healthcheck logic to return a success on invocation.  This has to be called by the service hosting this container to perform health checks.\n",
    "\n",
    "The request should be in the following format:\n",
    "\n",
    "`{\n",
    "  \"response_content_type\": \"<Specify either text/plain or application/json>\",\n",
    "  \"pred_x_csv\": \"<The comma-separated x column values to be used for prediction>\"\n",
    "}`\n",
    "\n",
    "This script will be packaged into the container that will be built in the upcoming steps.\n",
    "\n",
    "You can view the script by running the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the Python3 Flask script (containing the inference code)\n",
    "!cat {container_script_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085afc59",
   "metadata": {},
   "source": [
    "### D) Create the Dockerfile <a id='Create%20the%20Dockerfile'></a>\n",
    "\n",
    "In this step, we will create a [Dockerfile](https://docs.docker.com/engine/reference/builder/) which is required to build our [Docker](https://www.docker.com/) container containing the model pickle file, an inference script and its dependencies.\n",
    "\n",
    "In order to create the container, we will use the [Amazon Linux 2 container image](https://gallery.ecr.aws/amazonlinux/amazonlinux) available in the [Amazon ECR public registry](https://aws.amazon.com/ecr/) as the base image.  As this is a public registry, you do not require any credentials or permissions to download it.\n",
    "\n",
    "Note: At the time of writing this notebook, this image was based on [Amazon Linux 2](https://aws.amazon.com/amazon-linux-2/).  Depending on the specific version you intend to use, you can suffix container image URL with the specific version after the `:` character in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the inference script and requirements.txt to the container-artifacts directory\n",
    "!cp -pr {container_script_file} {container_artifacts_dir}/server.py\n",
    "!cp -pr {container_script_req_file} {container_artifacts_dir}/requirements.txt\n",
    "\n",
    "# Create the Dockerfile content\n",
    "dockerfile_content_lines = []\n",
    "dockerfile_content_lines.append('# syntax=docker/dockerfile:1\\n\\n')\n",
    "dockerfile_content_lines.append('# Use Amazon Linux 2 as the base image\\n')\n",
    "dockerfile_content_lines.append('FROM public.ecr.aws/amazonlinux/amazonlinux:latest\\n\\n')\n",
    "dockerfile_content_lines.append('# Setup the working directory\\n')\n",
    "dockerfile_content_lines.append('WORKDIR /\\n\\n')\n",
    "dockerfile_content_lines.append('# Install Python3\\n')\n",
    "dockerfile_content_lines.append('RUN yum -y install python3\\n\\n')\n",
    "dockerfile_content_lines.append('# Upgrade pip\\n')\n",
    "dockerfile_content_lines.append('RUN pip3 install --upgrade pip\\n\\n')\n",
    "dockerfile_content_lines.append('# Setup the Python virtual env to run the inference script\\n')\n",
    "dockerfile_content_lines.append('RUN python3 -m venv /opt/appenv\\n\\n')\n",
    "dockerfile_content_lines.append('# Install the Python packages required for the inference script in the virtual env\\n')\n",
    "dockerfile_content_lines.append('COPY requirements.txt .\\n')\n",
    "dockerfile_content_lines.append('RUN /opt/appenv/bin/pip install -r requirements.txt\\n\\n')\n",
    "dockerfile_content_lines.append('# Copy the extracted model file and the inference script\\n')\n",
    "dockerfile_content_lines.append('COPY ')\n",
    "dockerfile_content_lines.append(extracted_model_file_name)\n",
    "dockerfile_content_lines.append(' ./\\n')\n",
    "dockerfile_content_lines.append('COPY server.py ./\\n\\n')\n",
    "dockerfile_content_lines.append('# Specify the ENV variables\\n')\n",
    "dockerfile_content_lines.append('ENV MODEL_PICKLE_FILE_PATH=')\n",
    "dockerfile_content_lines.append(extracted_model_file_name)\n",
    "dockerfile_content_lines.append('\\n')\n",
    "dockerfile_content_lines.append('ENV FLASK_SERVER_LOG_LEVEL=DEBUG\\n')\n",
    "dockerfile_content_lines.append('ENV FLASK_SERVER_HOSTNAME=0.0.0.0\\n')\n",
    "dockerfile_content_lines.append('ENV FLASK_SERVER_PORT=')\n",
    "dockerfile_content_lines.append(str(ecs_container_port))\n",
    "dockerfile_content_lines.append('\\n')\n",
    "dockerfile_content_lines.append('ENV FLASK_SERVER_DEBUG=True\\n\\n')\n",
    "dockerfile_content_lines.append('# Specify the command to run the inference script as a Flask app\\n')\n",
    "dockerfile_content_lines.append('ENTRYPOINT [\"/opt/appenv/bin/python\", \"server.py\"]')\n",
    "\n",
    "# Create the Dockerfile\n",
    "dockerfile_local_path = '{}/Dockerfile'.format(container_artifacts_dir)\n",
    "with open(dockerfile_local_path, 'wt') as file:\n",
    "    file.write(''.join(dockerfile_content_lines))\n",
    "    \n",
    "# Print the contents of the generated Dockerfile\n",
    "!cat {dockerfile_local_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38eedde",
   "metadata": {},
   "source": [
    "### E) Create the container <a id='Create%20the%20container'></a>\n",
    "\n",
    "Create the Docker container using the `docker build` command.  Specify the container image name and point to the container-artifacts directory that contains all the files to build the container.\n",
    "\n",
    "Note: You may see warning messages when the container is built with the Dockerfile that we created in the prior step.  These warnings will be around installing the Python packages that are required by the inference script.  You can choose to either ignore or fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Docker container\n",
    "!docker build -t {container_image_name} {container_artifacts_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9c6a6",
   "metadata": {},
   "source": [
    "### F) Create the private repository in ECR <a id='Create%20the%20private%20repository%20in%20ECR'></a>\n",
    "\n",
    "In order to configure Amazon ECS to run a container, the container image should exist in a container registry.  In this notebook, we will create and use an [Amazon ECR](https://aws.amazon.com/ecr/) private repository for this purpose.\n",
    "\n",
    "In this step, we will check if the private repository in Amazon ECR that we intend to create already exists or not.  If it does not exist, we will create it with the repository name the same as the container image name.\n",
    "\n",
    "Note: When creating the repository, setting the `scanOnPush` parameter to `True` will automatically initiate a vulnerability scan on the container image that is pushed to the repository.  For more info on image scanning, refer [here](https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the ECR repository exists already; if not, then create it\n",
    "try:\n",
    "    ecr_client.describe_repositories(repositoryNames=[container_image_name])\n",
    "    print('ECR repository {} already exists.'.format(container_image_name))\n",
    "except ecr_client.exceptions.RepositoryNotFoundException:\n",
    "    print('ECR repository {} does not exist.'.format(container_image_name))\n",
    "    print('Creating ECR repository {}...'.format(container_image_name))\n",
    "    # Create the ECR repository - here we use the container image name for the repository name\n",
    "    ecr_client.create_repository(repositoryName=container_image_name,\n",
    "                                 imageScanningConfiguration={\n",
    "                                     'scanOnPush': True\n",
    "                                 })\n",
    "    print('Completed creating ECR repository {}.'.format(container_image_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b67906",
   "metadata": {},
   "source": [
    "### G) Push the container to ECR <a id='Push%20the%20container%20to%20ECR'></a>\n",
    "\n",
    "In this step, we will push the container to a private registry that we created in Amazon ECR.\n",
    "\n",
    "When using an Amazon ECR private registry, you must authenticate your Docker client to your private registry so that you can use the `docker push` and `docker pull` commands to push and pull images to and from the repositories in that registry.  For more information about this, refer [here](https://docs.aws.amazon.com/AmazonECR/latest/userguide/registry_auth.html).\n",
    "\n",
    "1. If this notebook instance is running on Amazon Linux v1, the authentication happens through an authorization token generated by an AWS CLI command in the following code cell.  This token will be automatically deleted when the code cell completes execution.\n",
    "2. If this notebook instance is running on Amazon Linux v2, the authentication happens through temporary credentials generated based on the IAM role attached to this notebook.  For this, you have to complete the prerequisite mentioned in the first step of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbd9f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image names\n",
    "source_image_name = '{}:latest'.format(container_image_name)\n",
    "target_image_name = '{}/{}:latest'.format(container_registry_url_prefix, container_image_name)\n",
    "\n",
    "if os_version == 'ALv1':\n",
    "    # Get the private registry credentials using an authorization token\n",
    "    !aws ecr get-login-password --region {region_name} | docker login --username AWS --password-stdin {container_registry_url_prefix}\n",
    "\n",
    "# Tag the container\n",
    "!docker tag {source_image_name} {target_image_name}\n",
    "\n",
    "# Push the container to the specified registry in Amazon ECR\n",
    "!docker push {target_image_name}\n",
    "\n",
    "if os_version == 'ALv1':\n",
    "    # Delete the Docker credentials file\n",
    "    print('\\nDeleting the generated Docker credentials file...')\n",
    "    !rm /home/ec2-user/.docker/config.json\n",
    "    print('Completed deleting the generated Docker credentials file.')\n",
    "    # Verify the delete\n",
    "    print('Verifying the delete of the generated Docker credentials file...')\n",
    "    !cat /home/ec2-user/.docker/config.json\n",
    "    print('Completed verifying the delete of the generated Docker credentials file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159b23b",
   "metadata": {},
   "source": [
    "##  5. Deploy and test on Amazon ECS on AWS Fargate <a id='Deploy%20and%20test%20on%20Amazon%20ECS%20on%20AWS%20Fargate'></a>\n",
    "\n",
    "In this step, we will create an [Amazon ECS cluster](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/clusters.html), deploy the Docker container that was created in the previous step as a [task](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html) on [Amazon ECS on AWS Fargate](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html) and test it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be2cc7",
   "metadata": {},
   "source": [
    "### A) Create the ECS cluster <a id='Create%20the%20ECS%20cluster'></a>\n",
    "\n",
    "In this step, we will check if the ECS cluster that we intend to create already exists or not.  If it does not exist, we will create it.\n",
    "\n",
    "Note:\n",
    "\n",
    "* We have not configured this cluster to use an [Amazon VPC](https://aws.amazon.com/vpc) for networking.  If you require it, refer to the instructions [here](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/create_cluster.html).\n",
    "* Sometimes, after the `delete_cluster` API is invoked on an ECS cluster, the cluster can go into 'INACTIVE' state and may remain discoverable in your AWS account for a period of time.  You may not see this in the AWS console.  When this happens, you won't be able to use the cluster.  For more information on this, refer [here](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ecs.html#ECS.Client.delete_cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693e003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the Amazon ECS cluster exists already; if not, then create it\n",
    "try:\n",
    "    describe_ecs_cluster_response = ecs_client.describe_clusters(clusters=[ecs_cluster_name])\n",
    "    if describe_ecs_cluster_response['failures'][0]['reason'] == 'MISSING':\n",
    "        print('ECS cluster {} does not exist.'.format(ecs_cluster_name))\n",
    "        print('Creating ECS cluster {}...'.format(ecs_cluster_name))\n",
    "        create_ecs_cluster_response = ecs_client.create_cluster(clusterName=ecs_cluster_name)\n",
    "        print('ECS cluster status = {}'.format(create_ecs_cluster_response['cluster']['status']))\n",
    "except IndexError:\n",
    "    ecs_cluster_status = describe_ecs_cluster_response['clusters'][0]['status']\n",
    "    print('ECS cluster \\'{}\\' already exists and is in status \\'{}\\'.'.format(ecs_cluster_name, ecs_cluster_status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54678e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep every 10 seconds and print the status of the ECS cluster until it goes to ACTIVE, INACTIVE or FAILED state\n",
    "while True:\n",
    "    describe_ecs_cluster_response = ecs_client.describe_clusters(clusters=[ecs_cluster_name])\n",
    "    ecs_cluster_status = describe_ecs_cluster_response['clusters'][0]['status']\n",
    "    print('ECS cluster status = {}'.format(ecs_cluster_status))\n",
    "    if ecs_cluster_status in {'ACTIVE', 'INACTIVE', 'FAILED'}:\n",
    "        break\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9749a93",
   "metadata": {},
   "source": [
    "### B) Create the ECS Task and deploy the container <a id='Create%20the%20ECS%20Task%20and%20deploy%20the%20container'></a>\n",
    "\n",
    "In this step, we will create a [task](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html) on [Amazon ECS on AWS Fargate](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html) and deploy the container.\n",
    "\n",
    "The following configuration will be used:\n",
    "\n",
    "* Fargate launch type.  For details on how to configure this, refer [here](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html).\n",
    "* The container location will be the Amazon ECR private registry that we created in prior steps.\n",
    "* The container port and host port will be what we configured in prior steps.\n",
    "* Protocol will be TCP.\n",
    "* `awslogs` driver will be used to send the logs to [Amazon CloudWatch](https://aws.amazon.com/cloudwatch/).\n",
    "* Healthcheck will be configured to invoke the healthcheck URL path suffix configured in the inference script's Flask app running in the container.\n",
    "* CPU and memory settings will be what we configured in prior steps.\n",
    "* The cluster name, container count, VPC subnets and Security Groups will be what we configured in prior steps.\n",
    "* Auto-assign Public IP address to the Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the ECS Fargate task definition\n",
    "ecs_register_task_definition_response = ecs_client.register_task_definition(family=ecs_fargate_task_name,\n",
    "                                                                            taskRoleArn=ecs_fargate_task_role,\n",
    "                                                                            executionRoleArn=ecs_fargate_task_execution_role,\n",
    "                                                                            networkMode='awsvpc',\n",
    "                                                                            containerDefinitions=[{\n",
    "                                                                                'name':ecs_container_name,\n",
    "                                                                                'image':target_image_name,\n",
    "                                                                                'portMappings': [{\n",
    "                                                                                    'containerPort':ecs_container_port,\n",
    "                                                                                    'hostPort':ecs_container_host_port,\n",
    "                                                                                    'protocol':'tcp',\n",
    "                                                                                }],\n",
    "                                                                                'logConfiguration': {\n",
    "                                                                                    'logDriver':'awslogs',\n",
    "                                                                                    'options': {\n",
    "                                                                                        'awslogs-create-group':'true',\n",
    "                                                                                        'awslogs-region':region_name,\n",
    "                                                                                        'awslogs-group':'/ecs/{}'.format(ecs_fargate_task_name),\n",
    "                                                                                        'awslogs-stream-prefix':'ecs'\n",
    "                                                                                    }\n",
    "                                                                                },\n",
    "                                                                                'healthCheck': {\n",
    "                                                                                    'command':ecs_container_healthcheck_command_list,\n",
    "                                                                                    'interval':ecs_container_healthcheck_interval_in_seconds,\n",
    "                                                                                    'timeout':ecs_container_healthcheck_timeout_in_seconds\n",
    "                                                                                }\n",
    "                                                                            }],\n",
    "                                                                            requiresCompatibilities=[\n",
    "                                                                                'FARGATE'\n",
    "                                                                            ],\n",
    "                                                                            cpu=ecs_fargate_task_cpu,\n",
    "                                                                            memory=ecs_fargate_task_memory\n",
    "                                                                           )\n",
    "\n",
    "\n",
    "# Print the task definition ARN\n",
    "ecs_fargate_task_definiton_arn = ecs_register_task_definition_response['taskDefinition']['taskDefinitionArn']\n",
    "print('ECS Fargate Task definition ARN = {}'.format(ecs_fargate_task_definiton_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7087168",
   "metadata": {},
   "source": [
    "The ECS cluster created in the previous step will take a few seconds to go into ACTIVE state.  Wait until then and proceed to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ECS Fargate task\n",
    "ecs_run_task_response = ecs_client.run_task(cluster=ecs_cluster_name,\n",
    "                                            count=ecs_fargate_task_count,\n",
    "                                            launchType='FARGATE',\n",
    "                                            networkConfiguration={\n",
    "                                                'awsvpcConfiguration':{\n",
    "                                                    'subnets':ecs_fargate_task_subnet_list,\n",
    "                                                    'securityGroups':ecs_fargate_task_security_group_list,\n",
    "                                                    'assignPublicIp': 'ENABLED'\n",
    "                                                }\n",
    "                                            },\n",
    "                                            taskDefinition=ecs_fargate_task_name)\n",
    "\n",
    "# Print the task ARN\n",
    "ecs_fargate_task_id = ecs_run_task_response['tasks'][0]['taskArn']\n",
    "print('ECS Fargate Task ARN = {}'.format(ecs_fargate_task_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f20d7",
   "metadata": {},
   "source": [
    "### C) Prepare to test the ECS Task <a id='Prepare%20to%20test%20the%20ECS%20Task'></a>\n",
    "\n",
    "Follow these steps to prepare to the test the ECS Task:\n",
    "\n",
    "1. **Wait for the Fargate task state to go to `RUNNING`** - after the successful run of the previous step, it will take few minutes for the ECS Fargate Task to go into `RUNNING` state.  You can check the state either using the AWS CLI/API/SDK or by going into the [AWS console](https://console.aws.amazon.com/ecs/home).  In the ECS console page for the AWS region, navigate to the specific ECS cluster that was created by this notebook and go to the Tasks tab.  There, you can check the state of your ECS Fargate Task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5751b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep every 5 seconds and print the status of the ECS Task until it goes to RUNNING or STOPPED state\n",
    "while True:\n",
    "    ecs_describe_tasks_response = ecs_client.describe_tasks(cluster=ecs_cluster_name,\n",
    "                                                            tasks=[ecs_fargate_task_id])\n",
    "    ecs_task_status = ecs_describe_tasks_response['tasks'][0]['lastStatus']\n",
    "    print('ECS Fargate Task status = {}'.format(ecs_task_status))\n",
    "    if ecs_task_status in {'RUNNING', 'STOPPED'}:\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964785ce",
   "metadata": {},
   "source": [
    "2. **Retrieve the Public IP address of this notebook instance** - if you intend to test the ECS Task from this notebook, then you will require the Public IP address of this notebook instance to configure in the next step.  You can retrieve it by running the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Public IP address of this notebook instance\n",
    "!curl ifconfig.me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6787d09",
   "metadata": {},
   "source": [
    "3. **Configure the Security Group on the ECS Task** - setup an Inbound Rule in the ECS Task's Security Group to allow access to the IP address of the system from where you are going to test the ECS Task.  This rule should be for the HTTP protocol on the configured port.  In this notebook, we have configured a Public IP address on the ECS Task on port 80.  We will test the ECS Task from this notebook.  So, make sure you configure the notebook instance's Public IP address retrieved in the previous step to configure this Security Group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5b7ca5",
   "metadata": {},
   "source": [
    "### D) Test the ECS Task <a id='Test%20the%20ECS%20Task'></a>\n",
    "\n",
    "In this step, we will test the ECS Task that we created in the previous step by invoking it synchronously.  For this, we will invoke the Python3 [Flask](https://flask.palletsprojects.com/en/1.1.x/) app script running in the container by using the Public IP address of the ECS Task.\n",
    "\n",
    "1. Retrieve the Public IP address of the ECS Task.\n",
    "2. Invoke the endpoint by making a HTTP POST call with the first record of the test dataset as a CSV string.\n",
    "    The request should be in the following format:\n",
    "    `{\n",
    "      \"response_content_type\": \"<Specify either text/plain or application/json>\",\n",
    "      \"pred_x_csv\": \"<The comma-separated x column values to be used for prediction>\"\n",
    "    }`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84d75af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the ECS Task details\n",
    "ecs_describe_tasks_response = ecs_client.describe_tasks(cluster=ecs_cluster_name,\n",
    "                                                       tasks=[ecs_fargate_task_id])\n",
    "\n",
    "# Retrieve the Public IP address of the ECS Task\n",
    "ecs_task_attachments = ecs_describe_tasks_response['tasks'][0]['attachments']\n",
    "for ecs_task_attachment in ecs_task_attachments:\n",
    "    if ecs_task_attachment['type'] == 'ElasticNetworkInterface':\n",
    "        ecs_task_attachment_details = ecs_task_attachment['details']\n",
    "        for ecs_task_attachment_detail in ecs_task_attachment_details:\n",
    "            if ecs_task_attachment_detail['name'] == 'networkInterfaceId':\n",
    "                ecs_task_nid = ecs_task_attachment_detail['value']\n",
    "                describe_network_interfaces_response = ec2_client.describe_network_interfaces(NetworkInterfaceIds=[\n",
    "                    ecs_task_nid\n",
    "                ])\n",
    "                ecs_fargate_task_public_ip = describe_network_interfaces_response['NetworkInterfaces'][0]['Association']['PublicIp']\n",
    "                \n",
    "# Print the Public IP address of the ECS Task\n",
    "print('ECS Task Public IP address = {}'.format(ecs_fargate_task_public_ip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the request payload\n",
    "x_test_request_payload_csv = ','.join(map(str, x_test[0]))\n",
    "x_test_request_payload = '{' + '\"response_content_type\": \"application/json\",\"pred_x_csv\":\"{}\"'.format(x_test_request_payload_csv) + '}'\n",
    "# Print the request\n",
    "print('Request payload:\\n')\n",
    "print(x_test_request_payload)\n",
    "\n",
    "# Invoke the ECS Task and print the response\n",
    "ecs_fargate_task_public_url = 'http://{}:80/'.format(ecs_fargate_task_public_ip)\n",
    "print('\\nResponse:\\n')\n",
    "!curl -X POST -H 'Content-Type: application/json' --data '{x_test_request_payload}' {ecs_fargate_task_public_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6213e2e9",
   "metadata": {},
   "source": [
    "##  6. (Optional) Front-end the container with Amazon API Gateway <a id='(Optional)%20Front-end%20the%20container%20with%20Amazon%20API%20Gateway'></a>\n",
    "\n",
    "For some use cases, you may prefer to front-end the inference as a service on Amazon ECS on AWS Fargate with [Amazon API Gateway](https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html).  With this setup, you can serve the model inference as an API with a HTTPS endpoint.  Prior to setting up the API, you have to create an [Amazon ECS Service](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html) made up of multiple tasks and then setup [Load Balancing](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html) and [Auto Scaling](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html).\n",
    "\n",
    "For the API, you have the following options to choose from:\n",
    "* [HTTP API](https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api.html)\n",
    "* [REST API](https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-rest-api.html)\n",
    "\n",
    "For guidance on choosing the right API option, refere [here](https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html).\n",
    "\n",
    "For information on setting up the container as the backend for Amazon API Gateway, refer [here](https://docs.aws.amazon.com/apigateway/latest/developerguide/setup-http-integrations.html).\n",
    "\n",
    "Note: The container that we created in prior steps has the logic to handle both REST and HTTP API requests from the Amazon API Gateway assuming the gateway passes through the request payload as-is to the backend container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c58a9",
   "metadata": {},
   "source": [
    "## 7. Cleanup <a id='Cleanup'></a>\n",
    "\n",
    "As a best practice, you should delete resources and S3 objects when no longer required.  This will help you avoid incurring unncessary costs.\n",
    "\n",
    "This step will cleanup the resources and S3 objects created by this notebook.\n",
    "\n",
    "Note: Apart from these resources, there will be Docker containers and related images created in the notebook instance that is running this Jupyter notebook.  As they are already part of the notebook instance, you do not need to delete them.  If you decide to delete them, then go to the Terminal of the Jupyter notebook and and run appropriate `docker` commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa9c64",
   "metadata": {},
   "source": [
    "### A) Cleanup ECS resources <a id='Cleanup%20ECS%20resources'></a>\n",
    "\n",
    "Note: Sometimes, after the `delete_cluster` API is invoked on an ECS cluster, the cluster can go into 'INACTIVE' state and may remain discoverable in your AWS account for a period of time.  You may not see this in the AWS console.  When this happens, you won't be able to use the cluster.  For more information on this, refer [here](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ecs.html#ECS.Client.delete_cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the ECS Task\n",
    "ecs_client.stop_task(cluster=ecs_cluster_name,\n",
    "                     task=ecs_fargate_task_id,\n",
    "                     reason='Cleanup from notebook {}'.format(nb_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3194a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deregister ECS Task definition\n",
    "ecs_client.deregister_task_definition(taskDefinition=ecs_fargate_task_definiton_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bc95e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the ECS cluster\n",
    "ecs_client.delete_cluster(cluster=ecs_cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f7a6b",
   "metadata": {},
   "source": [
    "### B) Cleanup ECR repository <a id='Cleanup%20ECR%20repository'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab5462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the ECR private repository\n",
    "try:\n",
    "    ecr_client.delete_repository(repositoryName=container_image_name, force=True)\n",
    "    print('ECR repository {} deleted.'.format(container_image_name))\n",
    "except ecr_client.exceptions.RepositoryNotFoundException:\n",
    "    print('ECR repository {} does not exist.'.format(container_image_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011884e7",
   "metadata": {},
   "source": [
    "### C) Cleanup S3 objects <a id='Cleanup%20S3%20objects'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00190629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete data from S3 bucket\n",
    "for file in s3_bucket_resource.objects.filter(Prefix='{}/'.format(nb_name)):\n",
    "    file_key = file.key\n",
    "    print('Deleting {} ...'.format(file_key))\n",
    "    s3_resource.Object(s3_bucket_resource.name, file_key).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414dad1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
